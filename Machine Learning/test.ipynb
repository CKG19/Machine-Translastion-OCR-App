{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Rubah coklat cepat melompat di atas anjing malas. ']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, T5ForConditionalGeneration\n",
    "\n",
    "# src_text = \"translate to Indonesia: \" + \"Here we build our detector and recognizer models. For both, weâ€™ll start with pretrained models. Note that for the recognizer, we freeze the weights in the backbone all the layers except for the final classification layer.\"\n",
    "src_text = \"translate to Indonesia: \" + \"The quick brown fox jumped over the lazy dog.\"\n",
    "# model_path = \"C:\\\\Users\\\\M-S-I\\\\Documents\\\\IBDA Semester 8\\\\Skripsi\\\\Machine Learning\\\\model\\\\flan-t5-finetuned-2\"\n",
    "model_path = \"muvazana/flan-t5-base-opus-en-id-id-en\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_path)\n",
    "\n",
    "# model_path = 'model/flan-t5-finetuned-en-to-id'\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "# model = AutoModelForSeq2SeqLM.from_pretrained(model_path)\n",
    "translated = model.generate(**tokenizer(src_text, return_tensors=\"pt\", padding=True), max_new_tokens=512)\n",
    "[tokenizer.decode(t, skip_special_tokens=True) for t in translated]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\M-S-I\\anaconda3\\envs\\ta-jacob\\lib\\site-packages\\transformers\\models\\marian\\tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Lambat kayu yang cepat melompatkan anjing yang lemah.']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "\n",
    "# src_text = \">>ind<< Here we build our detector and recognizer models. For both, weâ€™ll start with pretrained models. Note that for the recognizer, we freeze the weights in the backbone (all the layers except for the final classification layer).\"\n",
    "src_text = \">>ind<< The quick brown fox jumped on the lazy dog.\"\n",
    "# model_path = 'D:\\\\Users\\\\IBDA\\\\Documents\\\\TA Jacob\\\\model\\\\opus-mt-en-id-4'\n",
    "model_path = \"Helsinki-NLP/opus-mt-en-mul\"\n",
    "\n",
    "tokenizer = MarianTokenizer.from_pretrained(model_path)\n",
    "model = MarianMTModel.from_pretrained(model_path)\n",
    "\n",
    "translated = model.generate(**tokenizer(src_text, return_tensors=\"pt\", padding=True))\n",
    "[tokenizer.decode(t, skip_special_tokens=True) for t in translated]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Fulang coklat yang cepat melompat melintasi anjing malas itu.']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import MBart50TokenizerFast, MBartForConditionalGeneration\n",
    "\n",
    "# src_text = \"Here we build our detector and recognizer models. For both, weâ€™ll start with pretrained models. Note that for the recognizer, we freeze the weights in the backbone (all the layers except for the final classification layer).\"\n",
    "src_text = \"The quick brown fox jumped over the lazy dog.\"\n",
    "# model_path = 'model\\mbart-large-50-5'\n",
    "model_path = \"facebook/mbart-large-50-one-to-many-mmt\"\n",
    "\n",
    "tokenizer = MBart50TokenizerFast.from_pretrained(model_path,src_lang=\"en_XX\")\n",
    "model = MBartForConditionalGeneration.from_pretrained(model_path)\n",
    "\n",
    "model_inputs = tokenizer(src_text, return_tensors=\"pt\")\n",
    "generated_tokens = model.generate(**model_inputs,forced_bos_token_id=tokenizer.lang_code_to_id[\"id_ID\"], max_new_tokens=360)\n",
    "translation = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n",
    "translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.model_max_length"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ta-jacob",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
