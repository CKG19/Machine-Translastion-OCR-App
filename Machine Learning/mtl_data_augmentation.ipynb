{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    test: Dataset({\n",
       "        features: ['translation'],\n",
       "        num_rows: 2000\n",
       "    })\n",
       "    train: Dataset({\n",
       "        features: ['translation'],\n",
       "        num_rows: 1000000\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['translation'],\n",
       "        num_rows: 2000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "raw_datasets = load_dataset(\"Helsinki-NLP/opus-100\", \"en-id\")\n",
    "raw_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_mbart = 'facebook/mbart-large-50-one-to-many-mmt'\n",
    "\n",
    "from transformers import MBart50TokenizerFast\n",
    "tokenizer = MBart50TokenizerFast.from_pretrained(model_mbart, src_lang=\"en_XX\", tgt_lang=\"id_ID\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Augmentation #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "import string\n",
    "import random\n",
    "\n",
    "def ocr_error(text):\n",
    "  \"\"\"\n",
    "  Introduces random character errors (substitutions, insertions, deletions) to simulate OCR errors.\n",
    "  \"\"\"\n",
    "  probability = 0.1  # Adjust probability to control error frequency\n",
    "  tokenized_text = tokenizer.tokenize(text, max_length=512, truncation=True)\n",
    "  for i in range(len(tokenized_text)):  # Iterate by index\n",
    "    if random.random() < probability:\n",
    "      error_type = random.choice([\"substitute\", \"insert\", \"delete\"])\n",
    "\n",
    "      if tokenized_text and i < len(tokenized_text) and len(tokenized_text[i]) > 1:  # Check existence, index, and length\n",
    "        random_char_position = random.choice(range(len(tokenized_text[i])))\n",
    "      else:\n",
    "        continue  # Skip error simulation for empty or single-character tokens\n",
    "\n",
    "      if error_type == \"substitute\":\n",
    "        # Substitute with a random character, avoiding IndexError\n",
    "        if i < len(tokenized_text) - 1:  # Check remaining tokens\n",
    "          tokenized_text[i] = tokenized_text[i][:random_char_position] + \\\n",
    "                             random.choice(list(string.ascii_letters)) + \\\n",
    "                             tokenized_text[i][random_char_position+1:]\n",
    "      elif error_type == \"insert\":\n",
    "        # Insert a random character before the current character\n",
    "        tokenized_text[i] = tokenized_text[i][:random_char_position] + \\\n",
    "                             random.choice(list(string.ascii_letters)) + \\\n",
    "                             tokenized_text[i][random_char_position:]\n",
    "      elif error_type == \"delete\":\n",
    "        # Delete the current character but avoid empty list\n",
    "        if isinstance(tokenized_text[i], str):\n",
    "          # If it's a single-character string, skip deletion (avoid empty list)\n",
    "          continue\n",
    "        else:\n",
    "          tokenized_text[i] = tokenized_text[i][:random_char_position] + \\\n",
    "                              tokenized_text[i][random_char_position + 1:]\n",
    "\n",
    "  return \"\".join(tokenized_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Normalization #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def normalize_text(text):\n",
    "    text = text.lower()  # Lowercase\n",
    "    # text = re.sub(r\"[^\\w\\s]\", \"\", text)  # Remove non-alphanumeric characters (except whitespace)\n",
    "    # text = re.sub(r\"\\s+\", \" \", text)  # Replace excess whitespace with single space\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'en': 'Hut!', 'id': 'Pondok!'},\n",
       " {'en': 'Whoa, whoa, whoa.', 'id': 'Whoa, whoa, whoa.'},\n",
       " {'en': 'Cut it out.', 'id': 'Hentikan itu.'},\n",
       " {'en': 'The law is clear.', 'id': 'Hukumnya sangat jelas.'},\n",
       " {'en': 'You coming up?', 'id': 'Kau mau ikutan?'},\n",
       " {'en': 'Cheers.', 'id': 'Bersulang.'},\n",
       " {'en': 'And to Him belongs whoever is in the heavens and earth.',\n",
       "  'id': 'Dan kepunyaan-Nya-lah siapa saja yang ada di langit dan di bumi.'},\n",
       " {'en': 'From your pocket.', 'id': 'Dari sakumu.'},\n",
       " {'en': 'Mm.', 'id': 'Hmm.'},\n",
       " {'en': 'My men will attack his southern border.',\n",
       "  'id': 'Pasukanku akan menyerang perbatasan selatan negaranya.'}]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets[\"train\"][\"translation\"][10:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = [normalize_text(dt[\"en\"]) for dt in raw_datasets[\"train\"][\"translation\"]]\n",
    "targets = [normalize_text(dt[\"id\"]) for dt in raw_datasets[\"train\"][\"translation\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PreProcessing #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_lang = \"en\"\n",
    "target_lang = \"id\"\n",
    "def preprocess(data):\n",
    "  inputs = [dt[source_lang] for dt in data[\"translation\"]]\n",
    "  targets = [dt[target_lang] for dt in data[\"translation\"]]\n",
    "\n",
    "  # Apply augmentation to source language text\n",
    "  augmented_inputs = [ocr_error(text) for text in inputs]\n",
    "\n",
    "  model_inputs = tokenizer(augmented_inputs, truncation=True)\n",
    "\n",
    "  with tokenizer.as_target_tokenizer():\n",
    "    labels = tokenizer(targets, truncation=True)\n",
    "  model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "  return model_inputs\n",
    "\n",
    "tokenized_datasets = raw_datasets.map(preprocess, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(500000))\n",
    "small_eval_dataset = tokenized_datasets[\"test\"].shuffle(seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch translation with prefix #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import LineTokenizer\n",
    "import math\n",
    "\n",
    "model_path = '..\\Machine Learning\\model\\opus-mt-en-id-finetuned-en-to-id'\n",
    "tokenizer = MarianTokenizer.from_pretrained(model_path)\n",
    "model = MarianMTModel.from_pretrained(model_path)\n",
    "model = model.cuda()\n",
    "\n",
    "input_text = \"In the development of our translation application, we have decided to use a web-based approach. We will implement this application with a client-server architecture, where the client will interact with the application through a user-friendly User Interface (UI), while the server will handle requests from the client, process them, and return the results. We chose web development frameworks, such as Flask for the backend and Streamlit for the frontend, to speed up and simplify the application development process. The UI will be designed with a focus on ease of use and intuitive navigation, with responsive design in mind so that the application can be used comfortably on a variety of devices, including desktop and mobile devices.\"\n",
    "# input_text = \"In the development of our translation application, we have decided to use a web-based approach. We will implement this application with a client-server architecture, where the client will interact with the application through a user-friendly User Interface (UI), while the server will handle requests from the client, process them, and return the results. We chose a web development framework, Streamlit UI will be designed with a focus on ease of use and intuitive navigation, with responsive design in mind so that the application can be used comfortably on various devices, including desktop and mobile devices. Our app will offer three main services: OCR, Machine Translation (MT), and a combination of OCR & MT. Each service will have its own UI that allows users to select the desired model and submit input. For OCR services, users will be able to select the desired OCR model and upload PDF or Word format documents as input. The application will process the uploaded file using the selected OCR model and display the results to the user or allow the user to download the results. Next, for the Machine Translation (MT) service, the user will select the desired MT model and select the source language and target language for translation. The user will then enter text as input through the UI, and the application will translate the text using the selected MT model, displaying the results to the user. Lastly is the combination service (OCR â†’ MT), where the user will select the desired OCR and MT models, as well as the source language and target language for translation. The user will upload a PDF or Word format document as input, and the application will use the OCR model to recognize the text from the uploaded file, then translate the text using the selected MT model. The translation results will be displayed to the user or can be downloaded.\"\n",
    "\n",
    "lt = LineTokenizer()\n",
    "batch_size = 8\n",
    "\n",
    "paragraphs = lt.tokenize(input_text)\n",
    "translated_paragraphs = []\n",
    "prefix = \">>ind<< \"\n",
    "\n",
    "for paragraph in paragraphs:\n",
    "    sentences = sent_tokenize(paragraph)\n",
    "    batches = math.ceil(len(sentences) / batch_size)\n",
    "    translated = []\n",
    "    for i in range(batches):\n",
    "        sent_batch = [prefix + sentence for sentence in sentences[i*batch_size:(i+1)*batch_size]]\n",
    "        print(sent_batch)\n",
    "        print(len(sent_batch))\n",
    "        model_inputs = tokenizer(sent_batch, return_tensors=\"pt\", padding=True, truncation=True, max_length=500).to('cuda')\n",
    "        translated_batch = model.generate(**model_inputs)\n",
    "        translated += translated_batch\n",
    "    translated = tokenizer.batch_decode(translated, skip_special_tokens=True)\n",
    "    translated_paragraphs += [\" \".join(translated)]\n",
    "\n",
    "translated_text = \"\\n\".join(translated_paragraphs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ta-jacob",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
