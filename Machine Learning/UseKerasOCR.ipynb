{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import keras_ocr\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking for C:\\Users\\M-S-I\\.keras-ocr\\crnn_kurapan.h5\n",
      "Looking for C:\\Users\\M-S-I\\.keras-ocr\\craft_mlt_25k.h5\n",
      "1/1 [==============================] - 0s 425ms/step\n",
      "1/1 [==============================] - 2s 2s/step\n"
     ]
    }
   ],
   "source": [
    "recognizer_alphabet = \"0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ!\\\"#$%&'()*+,-./:;<=>?@[\\\\]^_`{|}~\"\n",
    "recogniser = keras_ocr.recognition.Recognizer()\n",
    "# recogniser.model.load_weights(f\"C:\\\\Users\\\\M-S-I\\\\Documents\\\\IBDA Semester 8\\\\Skripsi\\\\Machine Learning\\\\model\\\\keras_ocr_weights\\\\recognizer_cocotext_mid_freeze.h5\")\n",
    "\n",
    "# Initialize pipeline\n",
    "pipeline = keras_ocr.pipeline.Pipeline(recognizer=recogniser)\n",
    "\n",
    "# Read in image\n",
    "# image_paths = [\n",
    "#     \"C:\\\\Users\\\\M-S-I\\\\Documents\\\\IBDA Semester 8\\\\Skripsi\\\\Web\\\\pdf2png\\\\Proposal Proyek Akhir - Google Docs\\\\page_2.png\",\n",
    "#     \"C:\\\\Users\\\\M-S-I\\\\Documents\\\\IBDA Semester 8\\\\Skripsi\\\\Web\\\\pdf2png\\\\Proposal Proyek Akhir - Google Docs\\\\page_2.png\"\n",
    "# ]\n",
    "# images = [keras_ocr.tools.read(img) for img in image_paths]\n",
    "images = keras_ocr.tools.read(\"C:\\\\Users\\\\M-S-I\\\\Documents\\\\IBDA Semester 8\\\\OCR Image\\\\tes char.png\")\n",
    "\n",
    "# prediction_groups is a list of (word, box) tuples\n",
    "prediction_groups = pipeline.recognize([images])\n",
    "# prediction_groups[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(nrows=len(images), figsize=(20, 20))\n",
    "for ax, image, predictions in zip(axs, images, prediction_groups):\n",
    "    keras_ocr.tools.drawAnnotations(image=image, predictions=predictions, ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detections: ['dan', 'dunia', 'berputar', 'ditengah', 'semesta', 'alam', 'dan', 'luas', 'gelap', 'yang']\n"
     ]
    }
   ],
   "source": [
    "raw_detections = []\n",
    "for prediction in prediction_groups[0]:\n",
    "    raw_detections.append(prediction[0])\n",
    "print(f'Detections: {raw_detections}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_distance(predictions):\n",
    "    \"\"\"\n",
    "    Function returns dictionary with (key,value):\n",
    "        * text : detected text in image\n",
    "        * center_x : center of bounding box (x)\n",
    "        * center_y : center of bounding box (y)\n",
    "        * distance_from_origin : hypotenuse\n",
    "        * distance_y : distance between y and origin (0,0)\n",
    "    \"\"\"\n",
    "\n",
    "    # Point of origin\n",
    "    x0, y0 = 0, 0\n",
    "\n",
    "    # Generate dictionary\n",
    "    detections = []\n",
    "    for group in predictions:\n",
    "\n",
    "        # Get center point of bounding box\n",
    "        top_left_x, top_left_y = group[1][0]\n",
    "        bottom_right_x, bottom_right_y = group[1][1]\n",
    "        center_x, center_y = (top_left_x + bottom_right_x)/2, (top_left_y + bottom_right_y)/2\n",
    "\n",
    "        # Use the Pythagorean Theorem to solve for distance from origin\n",
    "        distance_from_origin = math.dist([x0,y0], [center_x, center_y])\n",
    "\n",
    "        # Calculate difference between y and origin to get unique rows\n",
    "        distance_y = center_y - y0\n",
    "\n",
    "        # Append all results\n",
    "        detections.append({\n",
    "                            'text': group[0],\n",
    "                            'center_x': center_x,\n",
    "                            'center_y': center_y,\n",
    "                            'distance_from_origin': distance_from_origin,\n",
    "                            'distance_y': distance_y\n",
    "                        })\n",
    "\n",
    "    return detections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = prediction_groups[0] # extract text list\n",
    "predictions = get_distance(predictions)\n",
    "# predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distinguish_rows(lst, thresh=15):\n",
    "    \"\"\"Function to help distinguish unique rows\"\"\"\n",
    "    sublists = []\n",
    "    for i in range(0, len(lst)-1):\n",
    "        if (lst[i+1]['distance_y'] - lst[i]['distance_y'] <= thresh):\n",
    "            if lst[i] not in sublists:\n",
    "                sublists.append(lst[i])\n",
    "            sublists.append(lst[i+1])\n",
    "        else:\n",
    "            yield sublists\n",
    "            sublists = [lst[i+1]]\n",
    "    yield sublists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'text': 'dan',\n",
       "   'center_x': 25.0,\n",
       "   'center_y': 4.0,\n",
       "   'distance_from_origin': 25.317977802344327,\n",
       "   'distance_y': 4.0},\n",
       "  {'text': 'dunia',\n",
       "   'center_x': 93.5,\n",
       "   'center_y': 5.0,\n",
       "   'distance_from_origin': 93.63359439859178,\n",
       "   'distance_y': 5.0},\n",
       "  {'text': 'berputar',\n",
       "   'center_x': 191.6936798095703,\n",
       "   'center_y': 5.60459041595459,\n",
       "   'distance_from_origin': 191.77559363137084,\n",
       "   'distance_y': 5.60459041595459},\n",
       "  {'text': 'ditengah',\n",
       "   'center_x': 305.0,\n",
       "   'center_y': 5.0,\n",
       "   'distance_from_origin': 305.0409808533929,\n",
       "   'distance_y': 5.0},\n",
       "  {'text': 'semesta',\n",
       "   'center_x': 417.4999694824219,\n",
       "   'center_y': 6.0,\n",
       "   'distance_from_origin': 417.54308103215317,\n",
       "   'distance_y': 6.0}],\n",
       " [{'text': 'alam',\n",
       "   'center_x': 29.5,\n",
       "   'center_y': 42.0,\n",
       "   'distance_from_origin': 51.324945202113945,\n",
       "   'distance_y': 42.0},\n",
       "  {'text': 'dan',\n",
       "   'center_x': 195.3096466064453,\n",
       "   'center_y': 42.66497802734375,\n",
       "   'distance_from_origin': 199.9153781168629,\n",
       "   'distance_y': 42.66497802734375},\n",
       "  {'text': 'luas',\n",
       "   'center_x': 144.0,\n",
       "   'center_y': 43.0,\n",
       "   'distance_from_origin': 150.28306624500314,\n",
       "   'distance_y': 43.0},\n",
       "  {'text': 'gelap',\n",
       "   'center_x': 252.0,\n",
       "   'center_y': 43.0,\n",
       "   'distance_from_origin': 255.64232826353307,\n",
       "   'distance_y': 43.0},\n",
       "  {'text': 'yang',\n",
       "   'center_x': 89.0,\n",
       "   'center_y': 44.0,\n",
       "   'distance_from_origin': 99.28242543370907,\n",
       "   'distance_y': 44.0}]]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set thresh higher for text further apart\n",
    "predictions = list(distinguish_rows(predictions, thresh=15))\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove all empty rows\n",
    "predictions = list(filter(lambda x:x!=[], predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detections: ['dan', 'dunia', 'berputar', 'ditengah', 'semesta', 'alam', 'yang', 'luas', 'dan', 'gelap']\n"
     ]
    }
   ],
   "source": [
    "# Order text detections in human readable format\n",
    "ordered_preds = []\n",
    "for row in predictions:\n",
    "    row = sorted(row, key=lambda x:x['distance_from_origin'])\n",
    "    for each in row: ordered_preds.append(each['text'])\n",
    "print(f'Detections: {ordered_preds}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dan dunia berputar ditengah semesta alam yang luas dan gelap\n"
     ]
    }
   ],
   "source": [
    "# Join detections into sentence\n",
    "chunk_size = 16\n",
    "result = \"\\n\".join([\" \".join(ordered_preds[i:i+chunk_size]) for i in range(0, len(ordered_preds), chunk_size)])\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
