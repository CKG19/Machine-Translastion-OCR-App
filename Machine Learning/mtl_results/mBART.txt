mBART_1: ~6 Hours
Freezing Encoders
dataset_rows=250.000
args = Seq2SeqTrainingArguments(
   f"mbart-large-50-one-to-many-mmt-finetuned-en-to-id",
   evaluation_strategy = "epoch",
   save_strategy= "epoch",
   learning_rate=1e-5,
   per_device_train_batch_size=16,
   per_device_eval_batch_size=16,
   weight_decay=0.01,
   save_total_limit=2,
   num_train_epochs=10,
   predict_with_generate=True,
   load_best_model_at_end= True,
   gradient_accumulation_steps=2,
)

Epoch,BLEU,METEOR,validation_loss
1,29.3351,0.5443,1.6552307605743408
2,29.3458,0.5431,1.6041151285171509
3,29.485,0.545,1.5822685956954956
4,29.3843,0.5466,1.5721431970596313
5,29.246,0.5403,1.5703024864196777
6,29.4905,0.5417,1.5738608837127686
7,29.5054,0.5416,1.5736815929412842

mBART_2: ~4 Hours 25 Minutes (stop at 5 epoch)
Freezing Encoders
dataset_rows=500.000
args = Seq2SeqTrainingArguments(
   f"mbart-large-50-one-to-many-mmt-finetuned-en-to-id",
   evaluation_strategy = "epoch",
   save_strategy= "epoch",
   learning_rate=3e-5,
   per_device_train_batch_size=16,
   per_device_eval_batch_size=16,
   weight_decay=0.01,
   save_total_limit=2,
   num_train_epochs=10,
   predict_with_generate=True,
   load_best_model_at_end= True,
   gradient_accumulation_steps=2,
)
Early_stopping_patience=2
Epoch,BLEU,METEOR,validation_loss
1,29.5254,0.545,1.5452399253845215
2,30.5962,0.5506,1.4987502098083496
3,30.1748,0.5497,1.4846129417419434
4,30.5429,0.5477,1.495391845703125
5,30.0845,0.5447,1.5172452926635742

mBART_3: ~6 Hours 14 Minutes (stop at 7 epoch)
Freezing Encoders
dataset_rows=500.000
args = Seq2SeqTrainingArguments(
   f"mbart-large-50-one-to-many-mmt-finetuned-en-to-id",
   evaluation_strategy = "epoch",
   save_strategy= "epoch",
   learning_rate=1e-5,
   per_device_train_batch_size=16,
   per_device_eval_batch_size=16,
   weight_decay=0.01,
   save_total_limit=10,
   num_train_epochs=10,
   predict_with_generate=True,
   load_best_model_at_end= True,
   gradient_accumulation_steps=2,
)
Early_stopping_patience=2
Epoch,BLEU,METEOR,validation_loss
1,29.83,0.5475,1.6006824970245361
2,30.063,0.5486,1.5458180904388428
3,29.8725,0.5458,1.523474097251892
4,30.2655,0.5484,1.5079424381256104
5,30.2659,0.5497,1.501773715019226
6,30.2993,0.5492,1.5020378828048706
7,30.2591,0.5486,1.5034472942352295

mbart_4: ~8 Hours 50 Minutes
Freezing Encoders
dataset_rows=500.000
args = Seq2SeqTrainingArguments(
   f"mbart-large-50-one-to-many-mmt-finetuned-en-to-id",
   evaluation_strategy = "epoch",
   save_strategy= "epoch",
   learning_rate=3e-5,
   per_device_train_batch_size=16,
   per_device_eval_batch_size=16,
   weight_decay=0.01,
   save_total_limit=10,
   num_train_epochs=10,
   predict_with_generate=True,
   load_best_model_at_end= True,
   gradient_accumulation_steps=2,
)
Epoch,BLEU,METEOR,validation_loss
1,29.5254,0.545,1.5452399253845215
2,30.5962,0.5506,1.4987502098083496
3,30.1748,0.5497,1.4846129417419434
4,30.5429,0.5477,1.495391845703125
5,30.0845,0.5447,1.5172452926635742
6,29.6011,0.539,1.5590629577636719
7,29.7952,0.5423,1.588382363319397
8,29.0214,0.5364,1.6141042709350586
9,29.1607,0.5363,1.6462734937667847
10,28.8031,0.5333,1.6634670495986938

mbart_5: ~8 Hours 50 Minutes
Freezing Encoders
dataset_rows=500.000
Dataset Normalization (lower case)
args = Seq2SeqTrainingArguments(
   f"mbart-large-50-one-to-many-mmt-finetuned-en-to-id",
   evaluation_strategy = "epoch",
   save_strategy= "epoch",
   learning_rate=1e-5,
   per_device_train_batch_size=16,
   per_device_eval_batch_size=16,
   weight_decay=0.01,
   save_total_limit=10,
   num_train_epochs=10,
   predict_with_generate=True,
   load_best_model_at_end= True,
   gradient_accumulation_steps=2,
)
Epoch,BLEU,METEOR,validation_loss
1,29.6097,0.5409,1.5810096263885498
2,30.1763,0.545,1.5280154943466187
3,30.3674,0.5492,1.5055866241455078
4,30.4685,0.5489,1.4970455169677734
5,30.9056,0.5503,1.4919929504394531
6,30.6871,0.5496,1.492728352546692
7,30.8259,0.5504,1.4924736022949219
8,30.6909,0.5499,1.4945107698440552
9,30.4868,0.5488,1.4983900785446167
10,30.4624,0.549,1.5002484321594238


5,30.9056,0.5503,1.4919929504394531
4,30.5962,0.5506,1.4987502098083496
3,30.2993,0.5492,1.5020378828048706
2,30.5962,0.5506,1.4987502098083496
1,29.5054,0.5416,1.5736815929412842